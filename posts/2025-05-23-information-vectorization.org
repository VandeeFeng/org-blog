#+title: AI 驱使下的信息向量化
#+date: <2025-05-23 19:51>
#+description: 未来的互联网正在逐步变成一个个 token，GitHub 已经被 [[https://deepwiki.com/][DeepWiki]] 变成 token 了，其他的还会远么？

#+filetags: Ramble

前两天的 [[https://io.google/2025/][Google I/O]] 有点炸裂，Google 这是掀桌子了，一下子放出了基于 Gemini 的整个生态的布局。这个网站整理了主要内容：https://google-io-2025.kcores.com/。

可以感觉到 Google 的战略和 OpenAI 是完全不一样的。基于 Google 庞大的生态应用，Google 正在逐步用 AI 接管这些，这次的 Google I／O 把蓝图完整的呈现了。

从这次的 Google I／O 结合最近在看的 [[https://book.douban.com/subject/35440637/][互联网口述历史第1辑·英雄创世记]] ，AI 对互联网内容和信息将带来的变革是颠覆性的。

* 从检索到生成
传统互联网上的搜索正在逐步从检索转变为生成。

作为长期处于传统搜索垄断地位的 Google，已经正式接入 AI 搜索了。这意味着以后的搜索可能就会变成今天的百度了。

互联网从诞生到现在积累了这么多年的信息和内容，可能真的马上要画上句号了。信息的价值只在它能够被检索到的时候才真正存在，再好的内容，不能被检索到，在互联网里它就相当于不存在了。

算法、流量这些现象同样也会慢慢发生在 AI 搜索。为了能够更好的让 AI 检索到，本来就越来越少的优质信息创作内容会更少。全自动 AI 生成，或半自动 AI 生成的信息一定会越来越多，传统的 SEO 也一定会向 AI 算法倾斜。

从内容到检索全部都被 AI 接管之后，不就变成百度了。

由于大语言模型本身的特性，不同于传统搜索引擎的关键字检索，如果不能在 prompt 里准确的描述出想搜索的内容，它一定会大话连篇的生成一大堆不太相关的东西。

从检索到生成，这就是我从 Google I/O 里感受最大的一点。以后在互联网上的搜索会逐步被 LLM 的生成所替代。

十年之后，各种 deep search 可能就真的替代现在的传统搜索了。

互联网上的信息正在逐步被向量化（embedded），从有血有肉的内容变成一个个 token。

* RSS 订阅
https://github.com/glidea/zenfeed/ ,最近看到的这个 GitHub 项目解决了我的大问题。它直接把 RSS 源内容的向量化，而不是像我之前用完整的 RSS 内容做 RAG。

在原项目的基础上，根据我自己的需求改了一下 https://github.com/vandeefeng/zenfeed/ ，现在用的相当舒服。现在可以直接用自然语言检索每天新闻里我感兴趣的内容了。

RSS 是我面对算法、信息茧房等问题坚持下来的信息获取习惯。从人出发，到信息，是我觉得面对现在 AI 技术带来的信息困境比较好的路径。这几年的 RSS 订阅，从 blog（点）穿起了这些 blog 的人（线），再到现在的他们辐射出的领域（面），积累了许多我觉得还算高质量的信息源。

算法、流量决定着我们现在能看到的大部分信息流，如果以后搜索真的也被 AI 全面接管了，检索也不行了，就只能从感兴趣的人出发了。

之前 [[https://www.vandee.art/2024-11-10-database-of-flowing-knowledge.html][数据库的搭建 - 流动知识检索]] 现在可以在这个项目的基础上继续发展了。这就是我以后的新闻信息检索入口。

从各大模型厂商推出 deep search 到现在，我也还是不想深入使用。数据源的质量自己不能控制，再怎么 deep 也不会是我想要的信息。当然，prompt 设计的好，还是可以筛选出很多高质量内容的。用多了就感觉有点厌烦了，直接 Google 一下反而神清气爽。

现在 deep search 一下，再用几个流行的 prompt 生成卡片网页、播客、甚至短视频，再发布到小红书、抖音、B 站、公众号等社交平台，整个工作流成本很低，也很容易复刻，也就导致大量的AI 生成的同质化内容。它们的质量参差不齐，我也实在不想浪费时间在这些信息上面。很多这些信息，啪啪啪一大堆，看起来也很美观，很有逻辑，但大多看完之后就像没看一样，数据的真实性就更不能保证了。

[[https://wiki.vandee.art/#%E6%99%BA%E5%8A%9B%E8%82%A5%E8%83%96%E5%8D%B1%E6%9C%BA][信息肥胖]]、信息健康这个概念很早就有了，只不过我感觉重视的人不太多。

* 数据向量化

虽然 AI 技术带来的很多变化我都不喜欢，但再不情愿也得接受。拒绝 AI 的这次技术浪潮在将来会带来的变革，就好像在说我今天就是不上网。

既然信息和数据在未来只有被向量化才能发挥更大的价值，那就先做好准备吧。

从我开始折腾 PKMS（个人知识管理）开始，就有这方面的考虑了。

数据始终是最核心的。再好的模型，没有优质的数据输入，也是白搭。

计划陆续要把 PKM 数据库里的内容全部向量化。大量数据的向量化优化，最近还在学习。


* 小结
未来的互联网正在逐步变成一个个 token，GitHub 已经被 [[https://deepwiki.com/][DeepWiki]] 变成 token 了，其他的还会远么？
