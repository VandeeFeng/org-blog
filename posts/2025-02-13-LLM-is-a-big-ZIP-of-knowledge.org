#+title: 或许可以这样看待大语言模型
#+date: <2025-02-13 15:14>
#+description: 是人类的想象能力让我们点燃了第一个知识的火把，在大自然严酷的生存法则里进化生存了下来，也是想象让我们不断走出柏拉图之穴探求知识和真理，也是想象让我们在苦难和痛苦中保持着乐观和希望。正是想象，让人类能够创造出属于我们自己的历史，把一个个幻想变成现实。
#+filetags: LLM Ramble

最近被 DeepSeek 刷屏了，我其实是有点烦的。这种过度的宣传，不可避免的会蒙蔽少部分凑热闹的人，但这就是热度本身。

本来是不太想写今天的内容的，但想着或许我这两年自学 AI 的经验可以帮助到哪些和我开始一样对 AI 感兴趣的人。正是因为我是一个 AI 行业的局外人，所以才能有不一样的视角。前几天的 [[https://www.vandee.art/2025-02-03-emacs-is-all-you-need.html][Emacs is all you need]] ，也完全是因为看到咸鱼上有人在炒作 DeepSeek 才写的。今天的也是类似，或许可以分享一些真正有用的东西。

我没有 AI 方面的专业背景，因此也不会涉及到专业的术语，只是一些学习的路径和思路，仅供参考。如果这些内容帮助到了你，我会很开心。

* 大语言模型是什么
看完 Andrej Karpathy 大佬的这两个视频：

[[https://www.youtube.com/watch?v=zjkBMFhNj_g&t=1218s][{1hr Talk} Intro to Large Language Models - YouTube]] , [[https://www.youtube.com/watch?v=7xTGNNLPyMI&t=7959s][Deep Dive into LLMs like ChatGPT - YouTube]]

基本上就对大语言模型 LLM 有了宏观上的概念了。

Transformer Neural Net 3D visualizer: https://bbycroft.net/llm ,这个网站用 3D 模型详细的展示了 transformer 架构的原理和过程。

用 AK 大佬的话来说，LLM 就是一个巨大的 ZIP，它汇聚了特定时间点互联网上所有的相对优质的知识和资源。这来源于在 pre-training 预训练过程中的数据集。之后的 post-training 不会改变 LLM 的基础知识（RAG 算是额外知识）。

本质上，LLM 就是在特定数据集的基础上，通过 transformer 架构神经网络来通过输入预测下一个词（现阶段流行的的大语言模型基本是基于 transformer 架构）。至于 transformer 架构是什么，可以简单理解为一个编码器和一个解码器。所有输入 LLM 的内容，最终会被 LLM 理解为一个个的 token, https://tiktokenizer.vercel.app/ 这个网站可以详细的看到输入的内容对于 LLM 来说是什么。

简单来说就是，一个个的 token 组成 sequence ，LLM 根据输入的内容，预测最可能的输出。平常计费的时候，消耗了多少多少 token ,也就是它。这就是发生在 LLM 对话框后面的简单过程。

上面的概念都来自与 AK 的视频，不是我自己的理解。

当然 LLM 背后的技术肯定比上面说的复杂，但实现的逻辑其实没那么难理解。

更多的关于 LLM 的基础知识，在 GitHub 上随便搜搜就有大量的合集。

推荐几个：
- [[id:3b9da3be-6fe2-496f-8563-6ecdbe36a672][Transformer模型图解-知乎]]，这篇文章很通俗详细的介绍了，transormer架构的原理。
- 李沐深度学习经典、新论文逐段精读 ：https://github.com/mli/paper-reading
- [[https://www.youtube.com/watch?v=9-Jl0dxWQs8][How might LLMs store facts | Chapter 7, Deep Learning - YouTube]] 3Blue1Brown 讲解 LLM 原理的视频，很优秀
- [[https://llmbook-zh.github.io/][大语言模型 | LLMBook-zh]]
- [[https://github.com/HandsOnLLM/Hands-On-Large-Language-Models][Hands-On-Large-Language-Models]]
- 这是一个简单的技术科普教程项目，主要聚焦于解释一些有趣的，前沿的技术概念和原理。每篇文章都力求在 5 分钟内阅读完成。

  https://github.com/karminski/one-small-step
* 或许可以这样看待大语言模型
** 知识库 ZIP
这部分是我自己的理解，仅仅仅供参考。

我一直是这么看待大语言模型的，它就是一个巨大的知识库，通过特定的 prompt，就可以拟人化的和这个知识库互动。

因此抛开 LLM 背后复杂的实现，在实际应用上，完全可以把它当作是自己的一个朋友、老师、私人助手。

我们不必要再去搜索许多专业的知识，因为在预训练的时候，LLM 这个 ZIP 里已经记录了，在预测的时候，它会优先从“记忆” 中回答，这也是容易产生幻觉的原因。当 LLM 在回答自己知识库之外的问题时，因为是预测的最可能的回答，它就会开始胡编乱造，也就是 hallucination。更何况，现在的 LLM 基于 Agent，可以调用 tool 来实现联网的搜索，让回答更加准确。

因此，在学习方面应用 LLM 就变成了，用特定的语句（prompt），去检索 LLM 的知识库，用特定的方式和这个朋友来互动。也难怪 OpenAI 会下那么大的力气在 AI 教育上，有了这个知识库，某种程度上，就有了一个 24 小时在线的老师。

这和看书其实没太大差别，只是从用手去翻书变成了用 prompt 检索问答。
** 自然语言编程
要说 LLM 在最开始对那个行业的冲击最大，那一定是程序员。简单来说，现在不必要必须掌握编程语言，在 LLM 的帮助下，就可以用自然语言进行编程。

因此，我也一直把 LLM 看作是一个新的编程语言。

对于普通人来说，编程可以解决什么问题呢？

如果说,编程的本质是将人类解决问题的思维（算法）转化为计算机可以执行的指令,赋予机器理解并执行复杂任务的能力,那么，LLM 极大的降低了这个过程的门槛。

我们现在的生活已经离不开互联网，更加离不开依附在互联网上的各种软件和服务，而这些都离不开计算机编程。

在 iPhone 上通过快捷指令截图识别内容、四象限日程提醒、新闻聚合筛选自己感兴趣的内容等等这些自定义的功能，在 LLM 的帮助下，普通人一个星期就可以开发出一个简单的 MVP( Minimum Viable Product 最小可行产品 ) 了。对于没有编程经验的人来说，LLM 大大降低了 MVP 开发的技术门槛和时间成本。

简单说就是，原本需要付费才能有的服务，现在自己动动手就可以平替了。

* 小结
LLM 拓宽了想象力的边界，这是我对 LLM 最简单的理解。

纵观人类历史的推动和发展，最离不开的就是想象力：从哥白尼的日心说，到爱因斯坦的相对论；从莱特兄弟的飞机到 spaceX；从图灵的图灵机，再到现在的大语言模型。

是人类的想象能力让我们点燃了第一个知识的火把，在大自然严酷的生存法则里进化生存了下来，也是想象让我们不断走出柏拉图之穴探求知识和真理，也是想象让我们在苦难和痛苦中保持着乐观和希望。正是想象，让人类能够创造出属于我们自己的历史，把一个个幻想变成现实。

LLM 是技术和工具, 决定上限的，始终是我们自身的思维能力。就像武林高手，即使手持柳枝也能削铁如泥。

Imagine and creating！对我来说，这是世界上最好玩的事情了。
