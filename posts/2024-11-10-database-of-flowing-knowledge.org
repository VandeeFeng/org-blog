#+title: 数据库的搭建 - 流动知识检索
#+date: <2024-11-10 18:36>
#+description:
#+filetags: PKM Github Python Database

Readwise 是我体验的最舒服的 after-reading 软件了，之前一直没有利用好它的 API，在 PKM 体系里，也一直没有统一到工作流。而这些流动知识的数据越来越多，以 10 年 20 年来度量，必须用数据库来完善和管理了，也是碰巧看到了这个项目：[[https://github.com/pocketbase/pocketbase][pocketbase/pocketbase: Open Source realtime backend in 1 file]] ,很简洁的管理 UI，支持 S3 数据备份，支持 API 导入和导出。

在 Claude 的协助下，有了现在的方案。

主要思路：利用 GitHub 为中转站， ~osmos::memos~ 插件保存文章，同步保存到 Readwise 进行阅读和高亮，再同步保存到 pocketbase 做数据库处理。

* 同步保存到 Readwise

现在使用 ~osmos::memos~ 插件保存的时候，会默认也保存到 readwise。添加 ~#2clip~ 标签触发 clip 的 workflow。详见： [[https://www.vandee.art/2024-10-12-bookmark-and-summary-by-github-actions.html][用 GitHub 仓库做书签和 AI 摘要 - 流动知识检索]]

这样就统一起来了。所有的链接都保存在 bookmark-collection 仓库作为中转站，然后统一在 Readwise 里阅读，导出 highlights。有保存价值的会触发 clip workflow 保存原文。

主要思路：在 push 的时候会运行这个 Python 脚本，读取新增内容，获取 URL 并使用 Readwise Reader 的 API 保存到 Readwise Reader。Readwise 和 Readwise Reader 是两个 API。

需要在仓库的 secret 里增加一个 ~READWISE_TOKEN~ secert，填入自己的 Readwise API。
** python

#+begin_src python
import os
import re
import logging
import requests
from github import Github

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

def get_added_content():
    """获取这次提交新增的内容"""
    try:
        g = Github(os.environ['GITHUB_TOKEN'])
        repo = g.get_repo(os.environ['GITHUB_REPOSITORY'])

        commit_sha = os.environ.get('GITHUB_SHA')
        commit = repo.get_commit(commit_sha)

        for file in commit.files:
            if file.filename.endswith('.md'):
                if file.patch and '+' in file.patch:
                    added_lines = [line[1:] for line in file.patch.split('\n')
                                 if line.startswith('+') and not line.startswith('+++')]
                    return added_lines
        return []

    except Exception as e:
        logging.error(f"Error getting commit content: {str(e)}")
        raise

def extract_url(line: str):
    """从行中提取URL，支持markdown格式的链接"""
    # 更新后的URL提取模式
    url_pattern = r'\((https?://[\w\-._~:/?#\[\]@!$&\'()*+,;=.%]+)\)'
    match = re.search(url_pattern, line)
    if match:
        return match.group(1)
    return None

def has_clip_tag(line: str) -> bool:
    """检查是否包含 #2clip 标签"""
    return bool(re.search(r'#2clip\b', line))

def trigger_workflow():
    """触发另一个workflow"""
    try:
        g = Github(os.environ['GITHUB_TOKEN'])
        repo = g.get_repo(os.environ['GITHUB_REPOSITORY'])

        workflow = repo.get_workflow("bookmark_summary.yml")
        workflow.create_dispatch("main")
        logging.info("Successfully triggered the bookmark_summary workflow")
    except Exception as e:
        logging.error(f"Failed to trigger workflow: {str(e)}")
        raise

def main():
    try:
        # 获取所有新增的内容
        added_lines = get_added_content()
        if not added_lines:
            logging.info("No new markdown content found in this commit")
            return

        trigger_needed = False

        # 处理每一行新增的内容
        for line in added_lines:
            line = line.strip()
            logging.info(f"Processing line: {line}")

            # 检查标签
            if has_clip_tag(line):
                trigger_needed = True
                logging.info("Found #2clip tag")

            # 提取并处理URL（无论是否有标签）
            url = extract_url(line)
            if url:
                try:
                    response = requests.post(
                        url="https://readwise.io/api/v3/save/",
                        headers={"Authorization": f"Token {os.environ['READWISE_TOKEN']}"},
                        json={
                            "url": url,
                            "tags": ["Bookmark"]
                        }
                    )
                    response.raise_for_status()
                    logging.info(f"Successfully saved URL: {url}")
                except requests.exceptions.RequestException as e:
                    logging.error(f"Failed to save URL {url}: {str(e)}")

        # 如果发现了标签，触发workflow
        if trigger_needed:
            logging.info("Triggering workflow due to #2clip tag")
            trigger_workflow()

    except Exception as e:
        logging.error(f"Error: {str(e)}")
        raise

if __name__ == "__main__":
    main()
#+end_src

** workflow

#+begin_src yaml

name: Save Bookmark to Readwise

on:
  push:
    branches:
      - main
    paths:
      - '**.md'
  workflow_dispatch:

permissions:
  contents: read
  actions: write

jobs:
  save-to-readwise:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests PyGithub

    - name: Run bookmark saver
      env:
        READWISE_TOKEN: ${{ secrets.READWISE_TOKEN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_REPOSITORY: ${{ github.repository }}
      run: python save_to_readwise.py

#+end_src

* PocketBase

需要在仓库的 secret 里增加 ~POCKETBASE_TOKEN~ 、 ~POCKETBASE_API~ 两个secert，填入自己的 pocketbase API 地址和请求头。

这个请求头我也是看了好半天文档才弄明白：

你需要在指定的 collection 的 API Rules 里，手动加上 ~@request.headers.x_token = "Your_token"~ ,这里填入的内容就是 ~POCKETBASE_TOKEN~ ， ~POCKETBASE_API~ 在每个 collection 里会显示。

由于这里指定了 header，在 python 的部分就得特别处理：

#+begin_src python

second_response = requests.post(
    url=os.environ['POCKETBASE_API'],
    headers={
        "x_token": f"{os.environ['POCKETBASE_TOKEN']}",
        "Content-Type": "application/json"
    },
    json={
        "URL": url,
        "title": title
    }
)

#+end_src

** docker 部署到 VPS

#+begin_src yaml

version: "3.7"
services:
  pocketbase:
    image: ghcr.io/muchobien/pocketbase:latest
    container_name: pocketbase
    restart: unless-stopped
    ports:
      - "8090:8090"
    volumes:
      - "./data:/pb_data"
    healthcheck: #optional (recommended) since v0.10.0
      test: wget --no-verbose --tries=1 --spider http://localhost:8090/api/health || exit 1
      interval: 5s
      timeout: 5s
      retries: 5

#+end_src
** Python

#+begin_src python

import os
import re
import logging
import requests
from github import Github

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

def get_added_content():
    """获取这次提交新增的内容"""
    try:
        g = Github(os.environ['GITHUB_TOKEN'])
        repo = g.get_repo(os.environ['GITHUB_REPOSITORY'])
        commit_sha = os.environ.get('GITHUB_SHA')
        commit = repo.get_commit(commit_sha)
        for file in commit.files:
            if file.filename.endswith('.md'):
                if file.patch and '+' in file.patch:
                    added_lines = [line[1:] for line in file.patch.split('\n')
                                 if line.startswith('+') and not line.startswith('+++')]
                    return added_lines
        return []
    except Exception as e:
        logging.error(f"Error getting commit content: {str(e)}")
        raise

def extract_url_and_title(line: str):
    """从行中提取URL和标题，支持markdown格式的链接 [title](url)"""
    # 更新后的提取模式，同时获取标题和URL
    pattern = r'\[(.*?)\]\((https?://[\w\-._~:/?#\[\]@!$&\'()*+,;=.%]+)\)'
    match = re.search(pattern, line)
    if match:
        title = match.group(1)
        url = match.group(2)
        return url, title
    return None, None

def has_clip_tag(line: str) -> bool:
    """检查是否包含 #2clip 标签"""
    return bool(re.search(r'#2clip\b', line))

def trigger_workflow():
    """触发另一个workflow"""
    try:
        g = Github(os.environ['GITHUB_TOKEN'])
        repo = g.get_repo(os.environ['GITHUB_REPOSITORY'])
        workflow = repo.get_workflow("bookmark_summary.yml")
        workflow.create_dispatch("main")
        logging.info("Successfully triggered the bookmark_summary workflow")
    except Exception as e:
        logging.error(f"Failed to trigger workflow: {str(e)}")
        raise

def main():
    try:
        # 获取所有新增的内容
        added_lines = get_added_content()
        if not added_lines:
            logging.info("No new markdown content found in this commit")
            return

        trigger_needed = False
        # 处理每一行新增的内容
        for line in added_lines:
            line = line.strip()
            logging.info(f"Processing line: {line}")

            # 检查标签
            if has_clip_tag(line):
                trigger_needed = True
                logging.info("Found #2clip tag")

            # 提取并处理URL和标题（无论是否有标签）
            url, title = extract_url_and_title(line)
            if url:
                try:
                    # 发送到 Readwise
                    response = requests.post(
                        url="https://readwise.io/api/v3/save/",
                        headers={"Authorization": f"Token {os.environ['READWISE_TOKEN']}"},
                        json={
                            "url": url,
                            "tags": ["Bookmark"]
                        }
                    )
                    response.raise_for_status()
                    logging.info(f"Successfully saved URL to Readwise: {url}")

                    # 发送到第二个 API endpoint
                    second_response = requests.post(
                        url=os.environ['POCKETBASE_API'],
                        headers={
                            "x_token": f"{os.environ['POCKETBASE_TOKEN']}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "URL": url,
                            "title": title
                        }
                    )
                    second_response.raise_for_status()
                    logging.info(f"Successfully saved URL to pocketbase: {url}")

                except requests.exceptions.RequestException as e:
                    logging.error(f"Failed to save URL {url}: {str(e)}")

        # 如果发现了标签，触发workflow
        if trigger_needed:
            logging.info("Triggering workflow due to #2clip tag")
            trigger_workflow()

    except Exception as e:
        logging.error(f"Error: {str(e)}")
        raise

if __name__ == "__main__":
    main()

#+end_src

** workflow

#+begin_src yaml
name: Save Bookmark to Readwise

on:
  push:
    branches:
      - main
    paths:
      - '**.md'
  workflow_dispatch:

permissions:
  contents: read
  actions: write

jobs:
  save-to-readwise:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests PyGithub

    - name: Run bookmark saver
      env:
        READWISE_TOKEN: ${{ secrets.READWISE_TOKEN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_REPOSITORY: ${{ github.repository }}
        POCKETBASE_API: ${{ secrets.POCKETBASE_API }}
        POCKETBASE_TOKEN: ${{ secrets.POCKETBASE_TOKEN }}
      run: python save_to_readwise.py

#+end_src

* Readwise highlights

写了一个 ~class ReadwiseAPI~ 方便其他项目引入。可以定时获取我所有 highlights 的 title 和 url。

后面可以直接把 highlights 导入到 pocketbase。
** python

#+begin_src python

import requests
import json
from datetime import datetime, timedelta
import os
from typing import List, Dict, Optional
from pathlib import Path
import re
from github import Github
import argparse

class ReadwiseAPI:
    """Readwise API client for exporting highlights with smart update capability and GitHub integration"""

    def __init__(self):
        # Initialize Readwise token
        self.readwise_token = os.environ.get("READWISE_TOKEN")
        if not self.readwise_token:
            raise ValueError("READWISE_TOKEN not found in environment variables")

        # Initialize GitHub token
        self.github_token = os.environ.get("GITHUB_TOKEN")
        if not self.github_token:
            raise ValueError("GITHUB_TOKEN not found in environment variables")

        # Get repository from GitHub Actions environment variable
        self.github_repo = os.environ.get("GITHUB_REPOSITORY")
        if not self.github_repo:
            raise ValueError("Not running in GitHub Actions environment (GITHUB_REPOSITORY not found)")

        # Initialize GitHub client
        self.github = Github(self.github_token)
        self.repo = self.github.get_repo(self.github_repo)

        # Initialize Readwise API settings
        self.base_url = "https://readwise.io/api/v2"
        self.headers = {
            "Authorization": f"Token {self.readwise_token}"
        }
        self.last_update_file = "last_update.json"
        self.articles_file = "articles.json"

    def get_highlights(self, updated_after: Optional[datetime] = None,
                      start_date: Optional[datetime] = None,
                      end_date: Optional[datetime] = None) -> Dict:
        """Get all highlights with their associated metadata"""
        endpoint = f"{self.base_url}/export/"
        params = {}

        if updated_after:
            params["updated_after"] = updated_after.isoformat()
        elif start_date:
            params["updated_after"] = start_date.isoformat()
            if end_date:
                params["updated_before"] = end_date.isoformat()

        print(f"Fetching highlights with params: {params}")
        response = requests.get(endpoint, headers=self.headers, params=params)
        response.raise_for_status()
        return response.json()

    def get_file_content(self, path: str) -> Optional[str]:
        """Get file content from GitHub repository"""
        try:
            content = self.repo.get_contents(path)
            return content.decoded_content.decode('utf-8')
        except Exception as e:
            print(f"File {path} not found in repository: {e}")
            return None

    def update_file(self, path: str, content: str, message: str):
        """Update or create file in GitHub repository"""
        try:
            # Try to get existing file
            file = self.repo.get_contents(path)
            # Update existing file
            self.repo.update_file(
                path=path,
                message=message,
                content=content,
                sha=file.sha
            )
        except Exception:
            # Create new file if it doesn't exist
            self.repo.create_file(
                path=path,
                message=message,
                content=content
            )

    def clean_title(self, title: str) -> str:
        """Clean title by removing newlines and extra spaces"""
        title = re.sub(r'\s+', ' ', title.replace('\n', ' '))
        return title.strip()

    def create_article_json(self, highlights_data: Dict) -> List[Dict]:
        """Create a list of articles with title and URL, only for category 'articles'"""
        articles = []

        for article in highlights_data.get('results', []):
            if article.get('category', '').lower() == 'articles':
                title = self.clean_title(article.get('title', 'Untitled'))
                url = article.get('source_url', '')

                articles.append({
                    'title': title,
                    'url': url
                })

        return articles

    def load_last_update_from_github(self) -> Optional[datetime]:
        """Load the last update date from GitHub"""
        content = self.get_file_content(self.last_update_file)
        if content:
            try:
                data = json.loads(content)
                return datetime.strptime(data['last_update'], '%Y-%m-%d')
            except Exception as e:
                print(f"Error parsing last update file: {e}")
                return None
        return None

    def save_last_update_to_github(self):
        """Save current date as last update date to GitHub"""
        current_date = datetime.now().strftime('%Y-%m-%d')
        content = json.dumps({'last_update': current_date})
        self.update_file(
            path=self.last_update_file,
            content=content,
            message="Update last sync date"
        )

    def load_existing_articles_from_github(self) -> List[Dict]:
        """Load existing articles from GitHub"""
        content = self.get_file_content(self.articles_file)
        if content:
            try:
                return json.loads(content)
            except Exception as e:
                print(f"Error parsing articles file: {e}")
                return []
        return []

    def merge_articles(self, existing_articles: List[Dict], new_articles: List[Dict]) -> List[Dict]:
        """Merge new articles with existing ones, avoiding duplicates"""
        existing_set = {(article['title'], article['url']) for article in existing_articles}

        for article in new_articles:
            article_tuple = (article['title'], article['url'])
            if article_tuple not in existing_set:
                existing_articles.append(article)
                existing_set.add(article_tuple)

        return existing_articles

    def export_articles(self, start_date: Optional[str] = None,
                       end_date: Optional[str] = None,
                       all_time: bool = False):
        """
        Export articles to GitHub with smart update capability

        Args:
            start_date: Optional start date in YYYY-MM-DD format
            end_date: Optional end date in YYYY-MM-DD format
            all_time: If True, fetch all highlights regardless of dates
        """
        if all_time:
            # 当选择 all_time 时，强制获取所有 highlights，忽略上次更新时间
            print("Fetching all highlights from the beginning")
            highlights_data = self.get_highlights()
        elif start_date:
            # 如果指定了开始日期，使用指定的日期范围
            start_datetime = datetime.strptime(start_date, '%Y-%m-%d')
            end_datetime = datetime.strptime(end_date, '%Y-%m-%d') if end_date else datetime.now()
            print(f"Fetching highlights from {start_date} to {end_date or 'now'}")
            highlights_data = self.get_highlights(start_date=start_datetime, end_date=end_datetime)
        else:
            # 使用上次更新时间的增量更新逻辑
            last_update = self.load_last_update_from_github()
            if last_update:
                days_since_update = (datetime.now() - last_update).days
                print(f"Last update was {days_since_update} days ago on {last_update.strftime('%Y-%m-%d')}")
                if days_since_update > 0:
                    print(f"Fetching highlights updated after {last_update.strftime('%Y-%m-%d')}")
                    highlights_data = self.get_highlights(updated_after=last_update)
                else:
                    print("Already updated today, no need to fetch new articles")
                    return
            else:
                print("No previous update found, fetching all articles")
                highlights_data = self.get_highlights()

        # Create article data
        new_articles = self.create_article_json(highlights_data)
        print(f"Found {len(new_articles)} new articles")

        # Load existing articles
        existing_articles = self.load_existing_articles_from_github()
        print(f"Found {len(existing_articles)} existing articles")

        # Merge new articles with existing ones
        merged_articles = self.merge_articles(existing_articles, new_articles)
        print(f"Total unique articles after merge: {len(merged_articles)}")

        # Save merged articles to GitHub
        self.update_file(
            path=self.articles_file,
            content=json.dumps(merged_articles, ensure_ascii=False, indent=2),
            message="Update articles list"
        )

        # Update the last update date
        if not start_date and not all_time:  # 只有在非指定日期范围和非全量更新的情况下才更新最后同步时间
            self.save_last_update_to_github()

        print(f"Successfully updated articles in GitHub repository")
        if new_articles:
            print("New articles added:")
            for article in new_articles:
                print(f"- {article['title']}")

def main():
    # 从环境变量获取 GitHub Actions 的输入参数
    gh_start_date = os.environ.get('INPUT_START_DATE', '')
    gh_end_date = os.environ.get('INPUT_END_DATE', '')
    gh_all_time = os.environ.get('INPUT_ALL_TIME', '').lower() == 'true'

    # 设置命令行参数解析器
    parser = argparse.ArgumentParser(description='Sync Readwise highlights to GitHub')
    parser.add_argument('--start-date', type=str, help='Start date in YYYY-MM-DD format')
    parser.add_argument('--end-date', type=str, help='End date in YYYY-MM-DD format')
    parser.add_argument('--all-time', action='store_true', help='Fetch all highlights from the beginning')

    args = parser.parse_args()

    # 优先使用命令行参数，如果没有则使用 GitHub Actions 的输入参数
    start_date = args.start_date or gh_start_date
    end_date = args.end_date or gh_end_date
    all_time = args.all_time or gh_all_time

    try:
        client = ReadwiseAPI()
        client.export_articles(
            start_date=start_date if start_date else None,
            end_date=end_date if end_date else None,
            all_time=all_time
        )
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        raise

if __name__ == "__main__":
    main()

#+end_src

** workflow

#+begin_src yaml

name: Sync Readwise Articles
on:
  schedule:
    # 每天凌晨 1 点运行 (UTC 时间，对应北京时间 9 点)
    - cron: '0 1 * * *'

  # 支持手动触发，并添加输入参数
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start date (YYYY-MM-DD, e.g., 2024-01-01)'
        required: false
        type: string
        default: ''
      end_date:
        description: 'End date (YYYY-MM-DD, leave empty for current date)'
        required: false
        type: string
        default: ''
      all_time:
        description: 'Fetch all highlights (overrides date range if selected)'
        type: boolean
        required: false
        default: false

permissions:
  contents: write      # 仓库内容的读写权限

jobs:
  sync:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        cache: 'pip'
        cache-dependency-path: '**/requirements.txt'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run sync script
      env:
        READWISE_TOKEN: ${{ secrets.READWISE_TOKEN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        INPUT_START_DATE: ${{ github.event.inputs.start_date }}
        INPUT_END_DATE: ${{ github.event.inputs.end_date }}
        INPUT_ALL_TIME: ${{ github.event.inputs.all_time }}
      run: python readwise_sync.py

    - name: Check for changes
      id: verify-changed-files
      run: |
        if [ -n "$(git status --porcelain)" ]; then
          echo "changes_found=true" >> $GITHUB_OUTPUT
        else
          echo "changes_found=false" >> $GITHUB_OUTPUT
        fi

    - name: Commit changes
      if: steps.verify-changed-files.outputs.changes_found == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add articles.json last_update.json
        git commit -m "Update Readwise articles [skip ci]" || echo "No changes to commit"

    - name: Push changes
      if: steps.verify-changed-files.outputs.changes_found == 'true'
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        branch: ${{ github.ref }}

#+end_src
